{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Gaussian Process Models in PyMC3\n",
    "\n",
    "**Chris Fonnesbeck**  \n",
    "*Vanderbilt University Medical Center*\n",
    "\n",
    "![](images/PyMC3.jpg)\n",
    "\n",
    "A common applied statistics task involves building regression models to characterize non-linear relationships between variables. It is possible to fit such models by assuming a particular non-linear structure, such as a sinusoidal, exponential, or polynomial function, to describe a given response by one variable to another. Unless this relationship is obvious from the outset, however, it involves possibly extensive model selection procedures to ensure the most appropriate model is retained. Alternatively, a non-parametric approach can be adopted by defining a set of knots across the variable space and use a spline or kernel regression to describe arbitrary non-linear relationships. However, knot layout procedures are somewhat *ad hoc* and can also involve variable selection. A third alternative is to adopt a **Bayesian non-parametric** strategy, and directly model the unknown underlying function. For this, we can employ Gaussian process models.\n",
    "\n",
    "![](images/points.png)\n",
    "\n",
    "Use of the term \"non-parametric\" in the context of Bayesian analysis is something of a misnomer. This is because the fundamental first step in Bayesian modeling is to specify a *full probability model* for the problem at hand, assigning probability densities to all unknown quantities of interest. So, it is difficult to explicitly state a full probability model without the use of probability functions, which are parametric! It turns out that Bayesian non-parametric methods do not imply that there are no parameters, but rather that the number of parameters grows with the size of the dataset. In fact, Bayesian non-parametric models are *infinitely* parametric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building models with Gaussians\n",
    "\n",
    "What if we chose to use Gaussian distributions to model our data? \n",
    "\n",
    "$$p(x \\mid \\pi, \\Sigma) = (2\\pi)^{-k/2}|\\Sigma|^{-1/2} \\exp\\left\\{ -\\frac{1}{2} (x-\\mu)^{\\prime}\\Sigma^{-1}(x-\\mu) \\right\\}$$\n",
    "\n",
    "There would not seem to be an advantage to doing this, because normal distributions are not particularly flexible distributions in and of themselves. However, adopting a set of Gaussians (a multivariate normal vector) confers a number of advantages. First, the marginal distribution of any subset of elements from  a multivariate normal distribution is also normal:\n",
    "\n",
    "$$p(x,y) = \\mathcal{N}\\left(\\left[{\n",
    "\\begin{array}{c}\n",
    "  {\\mu_x}  \\\\\n",
    "  {\\mu_y}  \\\\\n",
    "\\end{array}\n",
    "}\\right], \\left[{\n",
    "\\begin{array}{c}\n",
    "  {\\Sigma_x} & {\\Sigma_{xy}}  \\\\\n",
    "  {\\Sigma_{xy}^T} & {\\Sigma_y}  \\\\\n",
    "\\end{array}\n",
    "}\\right]\\right)$$\n",
    "\n",
    "$$p(x) = \\int p(x,y) dy = \\mathcal{N}(\\mu_x, \\Sigma_x)$$\n",
    "\n",
    "Also, conditionals distributions of a subset of a multivariate normal distribution (conditional on the remaining elements) are normal too:\n",
    "\n",
    "$$p(x|y) = \\mathcal{N}(\\mu_x + \\Sigma_{xy}\\Sigma_y^{-1}(y-\\mu_y), \n",
    "\\Sigma_x-\\Sigma_{xy}\\Sigma_y^{-1}\\Sigma_{xy}^T)$$\n",
    "\n",
    "A Gaussian process generalizes the multivariate normal to infinite dimension. It is defined as an infinite collection of random variables, any finite subset of which have a Gaussian distribution. Thus, the marginalization property is explicit in its definition. Another way of thinking about an infinite vector is as a *function*. When we write a function that takes continuous values as inputs, we are essentially specifying an infinte vector that only returns values (indexed by the inputs) when the function is called upon to do so. By the same token, this notion of an infinite-dimensional Gaussian as a function allows us to work with them computationally: we are never required to store all the elements of the Gaussian process, only to calculate them on demand.\n",
    "\n",
    "So, we can describe a Gaussian process as a ***disribution over functions***. Just as a multivariate normal distribution is completely specified by a mean vector and covariance matrix, a GP is fully specified by a mean *function* and a covariance *function*:\n",
    "\n",
    "$$p(x) \\sim \\mathcal{GP}(m(x), k(x,x^{\\prime}))$$\n",
    "\n",
    "It is the marginalization property that makes working with a Gaussian process feasible: we can marginalize over the infinitely-many variables that we are not interested in, or have not observed. \n",
    "\n",
    "For example, one specification of a GP might be as follows:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "m(x) &=0 \\\\\n",
    "k(x,x^{\\prime}) &= \\theta_1\\exp\\left(-\\frac{\\theta_2}{2}(x-x^{\\prime})^2\\right)\n",
    "\\end{aligned}$$\n",
    "\n",
    "here, the covariance function is a **squared exponential**, for which values of $x$ and $x^{\\prime}$ that are close together result in values of $k$ closer to 1 and those that are far apart return values closer to zero. It may seem odd to simply adopt the zero function to represent the mean function of the Gaussian process -- surely we can do better than that! It turns out that most of the learning in the GP involves the covariance function and its parameters, so very little is gained in specifying a complicated mean function.\n",
    "\n",
    "For a finite number of points, the GP becomes a multivariate normal, with the mean and covariance as the mean functon and covariance function evaluated at those points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from a Gaussian Process\n",
    "\n",
    "To make this notion of a \"distribution over functions\" more concrete, let's quickly demonstrate how we obtain realizations from a Gaussian process, which result in an evaluation of a function over a set of points. All we will do here is sample from the *prior* Gaussian process, so before any data have been introduced. What we need first is our covariance function, which will be the squared exponential, and a function to evaluate the covariance at given points (resulting in a covariance matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.cm as cmap\n",
    "sns.set_context('talk')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def exponential_cov(x, y, params):\n",
    "    return params[0] * np.exp( -0.5 * params[1] * np.subtract.outer(x, y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going generate realizations sequentially, point by point, using the lovely conditioning property of mutlivariate Gaussian distributions. Here is that conditional:\n",
    "\n",
    "$$p(x|y) = \\mathcal{N}(\\mu_x + \\Sigma_{xy}\\Sigma_y^{-1}(y-\\mu_y), \n",
    "\\Sigma_x-\\Sigma_{xy}\\Sigma_y^{-1}\\Sigma_{xy}^T)$$\n",
    "\n",
    "And this the function that implements it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional(x_new, x, y, params):\n",
    "    B = exponential_cov(x_new, x, params)\n",
    "    C = exponential_cov(x, x, params)\n",
    "    A = exponential_cov(x_new, x_new, params)\n",
    "    mu = np.linalg.inv(C).dot(B.T).T.dot(y)\n",
    "    sigma = A - B.dot(np.linalg.inv(C).dot(B.T))\n",
    "    return(mu.squeeze(), sigma.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with a Gaussian process prior with hyperparameters $\\theta_0=1, \\theta_1=10$. We will also assume a zero function as the mean, so we can plot a band that represents one standard deviation from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "θ = [1, 10]\n",
    "σ_0 = exponential_cov(0, 0, θ)\n",
    "xpts = np.arange(-3, 3, step=0.01)\n",
    "plt.errorbar(xpts, np.zeros(len(xpts)), yerr=σ_0, capsize=0)\n",
    "plt.ylim(-3, 3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select an arbitrary starting point to sample, say $x=1$. Since there are no prevous points, we can sample from an unconditional Gaussian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1.]\n",
    "y = [np.random.normal(scale=σ_0)]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now update our confidence band, given the point that we just sampled, using the covariance function to generate new point-wise intervals, conditional on the value $[x_0, y_0]$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "σ_1 = exponential_cov(x, x, θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, data, kernel, params, sigma, t):\n",
    "    k = [kernel(x, y, params) for y in data]\n",
    "    Sinv = np.linalg.inv(sigma)\n",
    "    y_pred = np.dot(k, Sinv).dot(t)\n",
    "    sigma_new = kernel(x, x, params) - np.dot(k, Sinv).dot(k)\n",
    "    return y_pred, sigma_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = np.linspace(-3, 3, 1000)\n",
    "predictions = [predict(i, x, exponential_cov, θ, σ_1, y) for i in x_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, sigmas = np.transpose(predictions)\n",
    "plt.errorbar(x_pred, y_pred, yerr=sigmas, capsize=0)\n",
    "plt.plot(x, y, \"ro\")\n",
    "plt.xlim(-3, 3); plt.ylim(-3, 3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So conditional on this point, and the covariance structure we have specified, we have essentially constrained the probable location of additional points. Let's now sample another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, s = conditional([-0.7], x, y, θ)\n",
    "y2 = np.random.normal(m, s)\n",
    "y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This point is added to the realization, and can be used to further update the location of the next point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.append(-0.7)\n",
    "y.append(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "σ_2 = exponential_cov(x, x, θ)\n",
    "\n",
    "predictions = [predict(i, x, exponential_cov, θ, σ_2, y) for i in x_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, sigmas = np.transpose(predictions)\n",
    "plt.errorbar(x_pred, y_pred, yerr=sigmas, capsize=0)\n",
    "plt.plot(x, y, \"ro\")\n",
    "plt.xlim(-3, 3); plt.ylim(-3, 3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, sampling sequentially is just a heuristic to demonstrate how the covariance structure works. We can just as easily sample several points at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_more = [-2.1, -1.5, 0.3, 1.8, 2.5]\n",
    "mu, s = conditional(x_more, x, y, θ)\n",
    "y_more = np.random.multivariate_normal(mu, s)\n",
    "y_more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x += x_more\n",
    "y += y_more.tolist()\n",
    "\n",
    "σ_new = exponential_cov(x, x, θ)\n",
    "\n",
    "predictions = [predict(i, x, exponential_cov, θ, σ_new, y) for i in x_pred]\n",
    "\n",
    "y_pred, sigmas = np.transpose(predictions)\n",
    "plt.errorbar(x_pred, y_pred, yerr=sigmas, capsize=0)\n",
    "plt.plot(x, y, \"ro\")\n",
    "plt.ylim(-3, 3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as the density of points becomes high, the result will be one realization (function) from the prior GP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Gaussian Processes in PyMC3\n",
    "\n",
    "Though it's entirely possible to extend the code above to introduce data and fit a Gaussian processes by hand, there are a number of libraries available for specifying and fitting GP models in a more automated way:\n",
    "\n",
    "- [scikit-learn](http://scikit-learn.org/stable/modules/gaussian_process.html)\n",
    "- [GPy](https://sheffieldml.github.io/GPy/)\n",
    "- [GPflow](http://gpflow.readthedocs.io/en/latest/intro.html)\n",
    "- [Stan](https://mc-stan.org/)\n",
    "- [Edward](edwardlib.org/)\n",
    "- [GPstuff](https://github.com/gpstuff-dev/gpstuff)\n",
    "\n",
    "I encourage you to experiment with several of these to compare their capabilities.\n",
    "\n",
    "This talk will focus on fitting Gaussian process models in PyMC3, using both simulated and real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyMC3\n",
    "\n",
    "The PyMC project is a very general Python package for probabilistic programming that can be used to fit nearly any Bayesian model. The current version (PyMC3) has been re-engineered from earlier versions to rely on a modern computational backend. PyMC3 is build on top of [Theano](http://deeplearning.net/software/theano/), an engine for evaluating expressions defined in terms of operations on tensors. It works in much the same way as TensorFlow, at least superficially, providing automatic differentiation, parallel computation, and dynamic generation of efficient, compiled code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "import theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic programming in Python confers a number of advantages including multi-platform compatibility, an expressive yet clean and readable syntax, easy integration with other scientific libraries, and extensibility via C, C++, Fortran or Cython. These features make it relatively straightforward to write and use custom statistical distributions, samplers and transformation functions, as required by Bayesian analysis.\n",
    "\n",
    "PyMC3's feature set helps to make Bayesian analysis as painless as possible. Here is a short list of some of its features:\n",
    "\n",
    "-   Fits Bayesian statistical models with Markov chain Monte Carlo, variational inference and\n",
    "    other algorithms.\n",
    "-   Includes a large suite of well-documented statistical distributions.\n",
    "-   Creates summaries including tables and plots.\n",
    "-   Several convergence diagnostics and model checking methods are available.\n",
    "-   Extensible: easily incorporates custom step methods and unusual probability distributions.\n",
    "-   MCMC loops can be embedded in larger programs, and results can be analyzed with the full power of Python.\n",
    "\n",
    "To give you an idea of what PyMC models look like, here is a short example using a parametric survival model. This is an exponential survival model for melanoma data, taken from Bayesian Survival Analysis (Ibrahim et al 2000):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i data/melanoma_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Normal, Model, DensityDist, sample\n",
    "from pymc3.math import log, exp\n",
    "\n",
    "with Model() as melanoma_survival:\n",
    "\n",
    "    # Convert censoring indicators to indicators for failure event\n",
    "    failure = (censored==0).astype(int)\n",
    "\n",
    "    # Parameters (intercept and treatment effect) for survival rate\n",
    "    μ = Normal('μ', 0, sd=100)\n",
    "    β = Normal('β', 0, sd=100)\n",
    "\n",
    "    # Survival rates, as a function of treatment\n",
    "    lam = exp(μ + β*treat)\n",
    "    \n",
    "    # Survival likelihood, accounting for censoring\n",
    "    def logp(failure, value):\n",
    "        return (failure * log(lam) - lam * value).sum()\n",
    "\n",
    "    x = DensityDist('x', logp, observed={'failure':failure, 'value':t})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example will generate 1000 posterior samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with melanoma_survival:\n",
    "    trace = sample(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyMC3 provides facilities for extracting and plotting model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import plot_posterior\n",
    "\n",
    "plot_posterior(trace, varnames=['β'], color='#a6bddb', ref_val=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gaussian processes in PyMC3\n",
    "\n",
    "The following simulated data clearly shows some type of non-linear process, corrupted by a certain amount of observation or measurement error so it should be a reasonable task for a Gaussian process approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run get_data.py\n",
    "sns.regplot(x, y, fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with the `fit` method, each supervised learning class retains a `predict` method that generates predicted outcomes ($y^*$) given a new set of predictors ($X^*$) distinct from those used to fit the model. For a Gaussian process, this is fulfulled by the *posterior predictive distribution*, which is the Gaussian process with the mean and covariance functions updated to their posterior forms, after having been fit. \n",
    "\n",
    "$$p(y^*|y, x, x^*) = \\mathcal{GP}(m^*(x^*), k^*(x^*))$$\n",
    "\n",
    "where the posterior mean and covariance functions are calculated as:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "m^*(x^*) &= k(x^*,x)^T[k(x,x) + \\sigma^2I]^{-1}y \\\\\n",
    "k^*(x^*) &= k(x^*,x^*)+\\sigma^2 - k(x^*,x)^T[k(x,x) + \\sigma^2I]^{-1}k(x^*,x)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance functions\n",
    "\n",
    "PyMC3 includes a library of covariance functions to choose from. A flexible choice to start with is the Mat&#232;rn covariance. \n",
    "\n",
    "$$k_{M}(x) = \\frac{\\sigma^2}{\\Gamma(\\nu)2^{\\nu-1}} \\left(\\frac{\\sqrt{2 \\nu} x}{l}\\right)^{\\nu} K_{\\nu}\\left(\\frac{\\sqrt{2 \\nu} x}{l}\\right)$$\n",
    "\n",
    "where where $\\Gamma$ is the gamma function and $K$ is a modified Bessel function. The form of covariance matrices sampled from this function is governed by three parameters, each of which controls a property of the covariance.\n",
    "\n",
    "* **amplitude** ($\\sigma$) controls the scaling of the output along the y-axis. This parameter is just a scalar multiplier, and is therefore usually left out of implementations of the Mat&#232;rn function (*i.e.* set to one)\n",
    "\n",
    "* **lengthscale** ($l$) complements the amplitude by scaling realizations on the x-axis. Larger values make points appear closer together.\n",
    "\n",
    "* **roughness** ($\\nu$) controls the sharpness of ridges in the covariance function, which ultimately affect the roughness (smoothness) of realizations.\n",
    "\n",
    "Though in general all the parameters are non-negative real-valued, when $\\nu = p + 1/2$ for integer-valued $p$, the function can be expressed partly as a polynomial function of order $p$ and generates realizations that are $p$-times differentiable, so values $\\nu \\in \\{3/2, 5/2\\}$ are extremely common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide an idea regarding the variety of forms or covariance functions, here's small selection of available ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(0,2,200)[:,None]\n",
    "\n",
    "# function to display covariance matrices\n",
    "def plot_cov(X, K, stationary=True):\n",
    "    K = K + 1e-8*np.eye(X.shape[0])\n",
    "    x = X.flatten()\n",
    "    \n",
    "    with sns.axes_style(\"white\"):\n",
    "\n",
    "        fig = plt.figure(figsize=(14,5))\n",
    "        ax1 = fig.add_subplot(121)\n",
    "        m = ax1.imshow(K, cmap=\"inferno\", \n",
    "                       interpolation='none', \n",
    "                       extent=(np.min(X), np.max(X), np.max(X), np.min(X))); \n",
    "        plt.colorbar(m);\n",
    "        ax1.set_title(\"Covariance Matrix\")\n",
    "        ax1.set_xlabel(\"X\")\n",
    "        ax1.set_ylabel(\"X\")\n",
    "\n",
    "        ax2 = fig.add_subplot(122)\n",
    "        if not stationary:\n",
    "            ax2.plot(x, np.diag(K), \"k\", lw=2, alpha=0.8)\n",
    "            ax2.set_title(\"The Diagonal of K\")\n",
    "            ax2.set_ylabel(\"k(x,x)\")\n",
    "        else:\n",
    "            ax2.plot(x, K[:,0], \"k\", lw=2, alpha=0.8)\n",
    "            ax2.set_title(\"K as a function of x - x'\")\n",
    "            ax2.set_ylabel(\"k(x,x')\")\n",
    "        ax2.set_xlabel(\"X\")\n",
    "\n",
    "        fig = plt.figure(figsize=(14,4))\n",
    "        ax = fig.add_subplot(111)\n",
    "        samples = np.random.multivariate_normal(np.zeros(200), K, 5).T;\n",
    "        for i in range(samples.shape[1]):\n",
    "            ax.plot(x, samples[:,i], color=cmap.inferno(i*0.2), lw=2);\n",
    "        ax.set_title(\"Samples from GP Prior\")\n",
    "        ax.set_xlabel(\"X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic exponential covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    l = 0.2 \n",
    "    tau = 2.0\n",
    "    b = 0.5\n",
    "    cov = b + tau * pm.gp.cov.ExpQuad(1, l)\n",
    "\n",
    "K = theano.function([], cov(X))()\n",
    "\n",
    "plot_cov(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matern $\\nu=3/2$ covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    l = 0.2\n",
    "    tau = 2.0\n",
    "    cov = tau * pm.gp.cov.Matern32(1, l)\n",
    "\n",
    "K = theano.function([], cov(X))()\n",
    "\n",
    "plot_cov(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    l = 0.2\n",
    "    tau = 2.0\n",
    "    cov = tau * pm.gp.cov.Cosine(1, l)\n",
    "\n",
    "K = theano.function([], cov(X))()\n",
    "\n",
    "plot_cov(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a general idea about covariance functions, let's begin by defining one for our first model.\n",
    "\n",
    "First, as a way of further introducing PyMC3, we will build the Gaussian process model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_distance(x, y): \n",
    "    return np.array([[(x[i] - y[j])**2 for i in range(len(x))] for j in range(len(y))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(y)\n",
    "\n",
    "with pm.Model() as gp_fit:\n",
    "    \n",
    "    μ = np.zeros(N)\n",
    "    \n",
    "    η_sq = pm.HalfCauchy('η_sq', 5)\n",
    "    ρ_sq = pm.HalfCauchy('ρ_sq', 5)\n",
    "    σ_sq = pm.HalfCauchy('σ_sq', 5)\n",
    "    \n",
    "    D = squared_distance(x, x)\n",
    "    \n",
    "    # Squared exponential\n",
    "    Σ = tt.fill_diagonal(η_sq * tt.exp(-ρ_sq * D), η_sq + σ_sq)\n",
    "    \n",
    "    obs = pm.MvNormal('obs', μ, Σ, observed=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from theano.tensor.nlinalg import matrix_inverse\n",
    "\n",
    "with gp_fit:\n",
    "    \n",
    "    # Prediction over grid\n",
    "    xgrid = np.linspace(-6, 6)\n",
    "    D_pred = squared_distance(xgrid, xgrid)\n",
    "    D_off_diag = squared_distance(x, xgrid)\n",
    "    \n",
    "    # Covariance matrices for prediction\n",
    "    Σ_pred = η_sq * tt.exp(-ρ_sq * D_pred)\n",
    "    Σ_off_diag = η_sq * tt.exp(-ρ_sq * D_off_diag)\n",
    "    \n",
    "    # Posterior mean\n",
    "    μ_post = pm.Deterministic('μ_post', tt.dot(tt.dot(Σ_off_diag, matrix_inverse(Σ)), y))\n",
    "    # Posterior covariance\n",
    "    Σ_post = pm.Deterministic('Σ_post', Σ_pred - tt.dot(tt.dot(Σ_off_diag, matrix_inverse(Σ)), Σ_off_diag.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will use **variational inference** to fit the model, namely automatic differentiation variational inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gp_fit:\n",
    "    approx = pm.fit(1000, method='fullrank_advi') \n",
    "    gp_trace = approx.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(gp_trace, varnames=['η_sq', 'ρ_sq', 'σ_sq']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [np.random.multivariate_normal(m, S) for m, S in zip(gp_trace['μ_post'], gp_trace['Σ_post'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for yp in y_pred:\n",
    "    plt.plot(np.linspace(-6, 6), yp, 'c-', alpha=0.1);\n",
    "plt.plot(x, y, 'r.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use PyMC's purpose-built GP classes to fit the same model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as gp_fit:\n",
    "\n",
    "    ρ = pm.HalfCauchy('ρ', 1)\n",
    "    η = pm.HalfCauchy('η', 1)\n",
    "    \n",
    "    K = η * pm.gp.cov.Matern32(1, ρ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue to build upon our model by speficying a mean function (this is redundant here, since a zero function is assumed when not specified) and an observation noise variable, which we will give a half-Cauchy prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with gp_fit:\n",
    "    \n",
    "    M = pm.gp.mean.Zero()\n",
    "    \n",
    "    σ = pm.HalfCauchy('σ', 2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian process model is encapsulated within the `GP` class, parameterized by the mean function, covariance function, and observation error specified above. Since the outcomes of the GP have been observed, we provide that data to the instance of `GP` in the `observed` argument as a dictionary. These are fed to the underlying multivariate normal likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gp_fit:\n",
    "    \n",
    "    y_obs = pm.gp.GP('y_obs', X=X, mean_func=M, cov_func=K, sigma2=σ**2, observed=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sample` function called inside the `Model` context fits the model using MCMC sampling. By default, PyMC3 uses an auto-tuning version of HMC called the [No U-turn Sampler](https://arxiv.org/abs/1111.4246) (NUTS) that picks appropriate values for the path length and step size parameters. Additionally, to initialize the sampler to reasonable starting parameter values, a variational inference algorithm is run before NUTS, to yield approximate posterior mean values for all the parametes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with gp_fit:\n",
    "    trace = pm.sample(1000, tune=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "axes = pm.traceplot(trace, varnames=['ρ', 'σ', 'η'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to fitting the model, we would like to be able to generate predictions. This implies sampling from the posterior predictive distribution, which if you recall is just some linear algebra:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "m^*(x^*) &= k(x^*,x)^T[k(x,x) + \\sigma^2I]^{-1}y \\\\\n",
    "k^*(x^*) &= k(x^*,x^*)+\\sigma^2 - k(x^*,x)^T[k(x,x) + \\sigma^2I]^{-1}k(x^*,x)\n",
    "\\end{aligned}$$\n",
    "\n",
    "PyMC3 allows for predictive sampling after the model is fit, using the recorded values of the model parameters to generate samples. The `sample_gp` function implements the predictive GP above, called with the sample trace, the GP variable and a grid of points over which to generate realizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.linspace(-6, 6, 100).reshape(-1, 1)\n",
    "with gp_fit:\n",
    "    gp_samples = pm.gp.sample_gp(trace, y_obs, Z, samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14,5))\n",
    "\n",
    "[ax.plot(Z, x, color='SeaGreen', alpha=0.3) for x in gp_samples]\n",
    "# overlay the observed data\n",
    "ax.plot(X, y, 'o', color=\"k\", ms=10);\n",
    "ax.set_xlabel(\"x\");\n",
    "ax.set_ylabel(\"f(x)\");\n",
    "ax.set_title(\"Posterior predictive distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For models being fit to very large datasets, one often finds MCMC fitting to work too slowly, as the log-probability of the model needs to be evaluated at every iteration of the sampling algorithm. In these situations, it may be worth using variational inference methods, which replace the true posterior with a simpler approximation, and use optimization to parameterize the approximation so that it is as close as possible to the target distribution. Thus, the posterior is only an approximation, and sometimes an unacceptably coarse one, but is a viable alternative for many problems. Newer variational inference algorithms are emerging that improve the quality of the approximation, and these will eventually find their way into software. In the meantime, [Variational Gaussian Approximation](http://www.mitpressjournals.org/doi/abs/10.1162/neco.2008.08-07-592#.WKNwR9aZOu4) and [Automatic Differentiation Variational Inference](https://arxiv.org/abs/1603.00788) are available now in GPflow and PyMC3, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the same model using just variational inference (ADVI):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gp_fit:\n",
    "    approx = pm.fit(20000)\n",
    "    trace_advi = approx.sample(1000, include_transformed=True)\n",
    "    gp_samples_advi = pm.gp.sample_gp(trace_advi, y_obs, Z, samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14,5))\n",
    "\n",
    "[ax.plot(Z, x, color='SeaGreen', alpha=0.3) for x in gp_samples_advi]\n",
    "# overlay the observed data\n",
    "ax.plot(X, y, 'o', color=\"k\", ms=10);\n",
    "ax.set_xlabel(\"x\");\n",
    "ax.set_ylabel(\"f(x)\");\n",
    "ax.set_title(\"Posterior predictive distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-world example: Spawning salmon\n",
    "\n",
    "That was contrived data; let's try applying Gaussian processes to a real problem. The plot below shows the relationship between the number of spawning salmon in a particular stream and the number of fry that are recruited into the population in the spring.\n",
    "\n",
    "We would like to model this relationship, which appears to be non-linear (we have biological knowledge that suggests it should be non-linear too).\n",
    "\n",
    "![](images/spawn.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salmon_data = pd.read_table('data/salmon.txt', sep='\\s+', index_col=0)\n",
    "salmon_data.plot.scatter(x='spawners', y='recruits', s=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as salmon_model:\n",
    "\n",
    "    ρ = pm.HalfCauchy('ρ', 3)\n",
    "    η = pm.HalfCauchy('η', 3)\n",
    "    \n",
    "    M = pm.gp.mean.Linear(coeffs=(salmon_data.recruits/salmon_data.spawners).mean())\n",
    "    K = η * pm.gp.cov.ExpQuad(1, ρ) \n",
    "    \n",
    "    σ = pm.HalfCauchy('σ', 2.5)\n",
    "    \n",
    "    recruits = pm.gp.GP('recruits', X=salmon_data.spawners.values.reshape(-1,1), \n",
    "                        mean_func=M, \n",
    "                        cov_func=K, \n",
    "                        sigma2=σ**2, \n",
    "                        observed=salmon_data.recruits.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with salmon_model:\n",
    "    salmon_trace = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salmon_pred = np.linspace(0, 500, 100).reshape(-1, 1)\n",
    "with salmon_model:\n",
    "    salmon_samples = pm.gp.sample_gp(salmon_trace, recruits, salmon_pred, samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = salmon_data.plot.scatter(x='spawners', y='recruits', c='k', s=50)\n",
    "for x in salmon_samples:\n",
    "    ax.plot(salmon_pred, x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might be interested in what may happen if the population gets very large -- say, 600 or 800 spawners. We can predict this, though it goes well outside the range of data that we have observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salmon_pred = np.linspace(0, 800, 100).reshape(-1, 1)\n",
    "with salmon_model:\n",
    "    pred = pm.gp.sample_gp(salmon_trace, recruits, salmon_pred, samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = salmon_data.plot.scatter(x='spawners', y='recruits', c='k', s=50)\n",
    "for x in pred:\n",
    "    ax.plot(salmon_pred, x, color='r', alpha=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian processes for classification\n",
    "\n",
    "The Gaussian process can be generalized to \n",
    "\n",
    "This example uses **elliptical slice sampling**, which can be used to perform inference in models with multivariate Gaussian priors (as we have with GPs). \n",
    "\n",
    "Its has a few advantages: \n",
    "\n",
    "1. it has simple, generic code applicable to many models\n",
    "2. it has no free parameters \n",
    "3. it works well for a variety of Gaussian process based models. \n",
    "\n",
    "These properties elliptical slice sampling useful, by removing the need to spend time deriving and tuning updates for more complex algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, I'm going to treat the hyperparameters as known, and concentrate on the slice sampling algorithm for estimating the predicted points from the Gaussian process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Number of training points\n",
    "n = 30\n",
    "X0 = np.sort(3 * np.random.rand(n))[:, None]\n",
    "\n",
    "# Number of points at which to interpolate\n",
    "m = 100\n",
    "X = np.linspace(0, 3, m)[:, None]\n",
    "\n",
    "# Covariance kernel parameters\n",
    "noise = 0.1\n",
    "lengthscale = 0.3\n",
    "f_scale = 1\n",
    "\n",
    "# Covariance matrices\n",
    "cov = f_scale * pm.gp.cov.ExpQuad(1, lengthscale)\n",
    "K = cov(X0)\n",
    "K_star = cov(X0, X)\n",
    "K_noise = K + noise * np.eye(n)\n",
    "\n",
    "# Perturbation to the covariance matrix diagonal to improve numerical stability\n",
    "K_stable = K + 1e-3 * np.eye(n)\n",
    "\n",
    "# Observed data\n",
    "f = np.random.multivariate_normal(mean=np.zeros(n), cov=K_noise.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.random.multivariate_normal(mean=np.zeros(n), cov=K_stable.eval())\n",
    "\n",
    "# Separate data into positive and negative classes\n",
    "f[f > 0] = 1\n",
    "f[f <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 6));\n",
    "ax.scatter(X0, np.ma.masked_where(f == 0, f), color='b', label='Positive Observations');\n",
    "ax.scatter(X0, np.ma.masked_where(f == 1, f), color='r', label='Negative Observations');\n",
    "ax.legend(loc='lower right');\n",
    "ax.set_xlim(-0.1, 3.1);\n",
    "ax.set_ylim(-0.2, 1.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     f_scale = pm.HalfCauchy('f_scale', 1)\n",
    "#     lengthscale = pm.HalfCauchy('lengthscale', 1)\n",
    "#     sigma = pm.HalfCauchy('sigma', 1)\n",
    "    \n",
    "#     cov = f_scale * pm.gp.cov.ExpQuad(1, lengthscale)\n",
    "#     K = cov(X0)\n",
    "#     K_star = cov(X0, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    \n",
    "    # f_sample is just a dummy variable\n",
    "    f_sample = pm.Flat('f_sample', shape=n)\n",
    "    f_transform = pm.invlogit(f_sample)\n",
    "    \n",
    "    # Binomial likelihood\n",
    "    y = pm.Bernoulli('y', p=f_transform, shape=n, observed=f)\n",
    "\n",
    "    # Interpolate function values using noiseless covariance matrix\n",
    "    L = tt.slinalg.cholesky(K_stable)\n",
    "    f_pred = pm.Deterministic('f_pred', tt.dot(K_star.T, tt.slinalg.solve(L.T, tt.slinalg.solve(L, f_transform))))\n",
    "\n",
    "    # Use elliptical slice sampling\n",
    "    ess_step = pm.EllipticalSlice(vars=[f_sample], prior_cov=K_stable)\n",
    "    trace = pm.sample(5000, start=model.test_point, step=[ess_step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 6));\n",
    "for ypred in trace['f_pred'][-50:]:\n",
    "    ax.plot(X, ypred,  alpha=0.04, color='SeaGreen')\n",
    "ax.scatter(X0, np.ma.masked_where(f == 0, f), color='b', label='Positive Observations');\n",
    "ax.scatter(X0, np.ma.masked_where(f == 1, f), color='r', label='Negative Observations');\n",
    "ax.legend(loc='lower right');\n",
    "ax.set_xlim(-0.1, 3.1);\n",
    "ax.set_ylim(-0.2, 1.2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Gaussian processes\n",
    "\n",
    "One of the major constraints that limits the utility of Gaussian processes in practice is the inversion of $K$ when calculating the posterior covariance. Since it is evaluated at every observed data point, its execution time is $\\mathcal{O(n^3)}$, which makes Gaussian processes (in the form I have presented here) impractical for larger datasets.\n",
    "\n",
    "An approach for dealing with this computation complexity is to look for an approximation to accelerate training and prediction. For Gaussian processes, this can be accomplished by employing a **sparse approximation** to the Gram matrix that places $M<<N$ inducing points along the range of the input variables, and uses this to estimate the full covariance matrix for the observed points. The *fully independent training conditional (FITC)* algorithm can then be used to estimate the model, with the critical approximation being the imposition of a conditional independence assumption on the joint prior over training and test cases.\n",
    "\n",
    "The FITC approximation is currently being developed for PyMC3 as part of the Google Summer of Code (GSoC) program. Here is a preview of this approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace.varnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inducing points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run get_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xu = np.linspace(-5, 5, 10)[:,None]\n",
    "Z = np.linspace(-6, 6, 100).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X.flatten(), y.flatten(), 'ko');\n",
    "\n",
    "plt.plot(Xu.flatten(), -2 * np.ones(Xu.shape[0]), \"X\", color='r', ms=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct and fit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    l = pm.HalfCauchy(\"l\", beta=5, testval=0.2)\n",
    "    sf2 = pm.HalfCauchy(\"sf2\", beta=5, testval=1.0)\n",
    "    sn2 = pm.HalfCauchy(\"sn2\", beta=5, testval=0.2)\n",
    "    \n",
    "    cov = pm.gp.cov.ExpQuad(1, l) * sf2\n",
    "    mu = pm.gp.mean.Zero()\n",
    "    \n",
    "    # with inducing points\n",
    "    gp_u = pm.gp.gp.GPfitc(\"gp_u\", mean_func=mu, cov_func=cov, X=X, inducing_points=Xu, sigma2=sn2, observed=y)\n",
    "    \n",
    "    # full GP\n",
    "    gp_full = pm.gp.gp.GP(\"gp_full\", mean_func=mu, cov_func=cov, X=X, sigma2=sn2, observed=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    start = pm.find_MAP(live_disp=True, fmin=sp.optimize.fmin_l_bfgs_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw samples from posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    samples_u = pm.gp.gp.sample_gp([start]*15, gp_u, Z, obs_noise=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True model, for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    samples_full = pm.gp.gp.sample_gp([start]*15, gp_full, Z, obs_noise=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X.flatten(), y.flatten(), 'ko');\n",
    "\n",
    "plt.plot(Z.flatten(), samples_u.T, 'Navy', alpha=0.4);\n",
    "plt.plot(Z.flatten(), samples_full.T, 'SeaGreen', alpha=0.3);\n",
    "plt.title(\"Posterior\")\n",
    "    \n",
    "plt.plot(Xu.flatten(), -2 * np.ones(Xu.shape[0]), \"X\", color='r', ms=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we want to be able to use this for a much larger analysis, in terms of the number of observations used to fit the model. Here is a denser simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = 2000\n",
    "x = np.linspace(0,1,nx)\n",
    "y = x*x * np.sin(2*np.pi*1.5*x) + 0.1*np.random.randn(nx)\n",
    "y = y - np.mean(y)\n",
    "plt.plot(x, y, 'o');\n",
    "\n",
    "Z = np.linspace(0,1,100)[:,None]\n",
    "Xu = np.linspace(0,1,6)[:,None]\n",
    "X = x[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    l = pm.HalfCauchy(\"l\", beta=5, testval=0.2)\n",
    "    sf2 = pm.HalfCauchy(\"sf2\", beta=5, testval=1.0)\n",
    "    sn2 = pm.HalfCauchy(\"sn2\", beta=5, testval=0.2)\n",
    "    \n",
    "    cov = pm.gp.cov.ExpQuad(1, l) * sf2\n",
    "    mu = pm.gp.mean.Zero()\n",
    "    \n",
    "    # with inducing points\n",
    "    gp = pm.gp.gp.GPfitc(\"gp\", mean_func=mu, cov_func=cov, X=X, inducing_points=Xu, sigma2=sn2, observed=y)\n",
    "\n",
    "with model:\n",
    "    approx = pm.find_MAP(live_disp=True, fmin=sp.optimize.fmin_l_bfgs_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    samples = pm.gp.gp.sample_gp([approx]*5, gp, Z, progressbar=True, obs_noise=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X.flatten(), y.flatten(), 'o', color='SeaGreen', alpha=0.4);\n",
    "\n",
    "plt.plot(Z.flatten(), samples.T, color='k', alpha=0.6)\n",
    "    \n",
    "plt.plot(Xu.flatten(), -0.5 * np.ones(Xu.shape[0]), \"X\", color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multidimensional GP\n",
    "\n",
    "Until now, our examples have been of 1-dimensional Gaussian processes, where there is just a single predictor variable thought to have a non-linear relationship to the outcome. Let's look at a real-world dataset that involves two predictors. We will use the famous **Walker Lake dataset (Isaaks & Srivistava 1989)** that involves spatial sampling of minerals and other variables over space. The data consist of two spatial coordinates and three measured outcomes. The outcomes are anonymously labeled as U, V (continuous variables, such as concentrarion) and T (discrete variable, such as the presence of a particular element). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walker_data = pd.read_table('data/walker.txt', sep='\\s+', index_col=0, skiprows=8, header=None, \n",
    "              names=['ID', 'Xloc', 'Yloc', 'V', 'U', 'T'])\n",
    "walker_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The samples are taken regularly over a coarse grid across the entire area, and then irregularly over portions of the area, presumably where there were positive samples on the coarser grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = 40\n",
    "x1, x2 = np.meshgrid(np.linspace(0,300,nx), np.linspace(0,300,nx))\n",
    "X = np.concatenate([x1.reshape(nx*nx, 1), x2.reshape(nx*nx, 1)], 1)\n",
    "\n",
    "X_obs = walker_data[['Xloc', 'Yloc']].values\n",
    "y_obs = walker_data.V.values\n",
    "\n",
    "with sns.axes_style(\"white\"):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.scatter(X_obs[:,0], X_obs[:,1], s=50, c=y_obs, marker='s', cmap=plt.cm.viridis);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a grid to predict on, as well as a sparser grid of inducing points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd = 30\n",
    "z1, z2 = np.meshgrid(np.linspace(0, 300, nd), np.linspace(0, 300, nd))\n",
    "Z = np.concatenate([z1.reshape(nd*nd, 1), z2.reshape(nd*nd, 1)], 1)\n",
    "\n",
    "nd = 15\n",
    "xu1, xu2 = np.meshgrid(np.linspace(0, 300, nd), np.linspace(0, 300, nd))\n",
    "Xu = np.concatenate([xu1.reshape(nd*nd, 1), xu2.reshape(nd*nd, 1)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    l = pm.HalfCauchy(\"l\", beta=5, shape=(2,), testval=np.array([0.2, 0.4]))\n",
    "    sf2 = pm.HalfCauchy(\"sf2\", beta=5, testval=1.0)\n",
    "    sn2 = pm.HalfCauchy(\"sn2\", beta=5, testval=0.2)\n",
    "\n",
    "    cov = pm.gp.cov.ExpQuad(2, l) * sf2\n",
    "    mu = pm.gp.mean.Zero()\n",
    "    gp = pm.gp.gp.GPfitc(\"yobs\", mean_func=mu, cov_func=cov, X=X_obs, inducing_points=Xu, sigma2=sn2, observed=y_obs)\n",
    "\n",
    "with model:\n",
    "    start = pm.find_MAP(live_disp=True, fmin=sp.optimize.fmin_l_bfgs_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    samples = pm.gp.gp.sample_gp([start]*2, gp, Z, obs_noise=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.scatter(z1.flatten(), z2.flatten(), s=100, c=samples[0,:], marker='s', cmap=plt.cm.viridis);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Closing\n",
    "\n",
    "Python users are incredibly lucky to have so many options for constructing and fitting non-parametric regression and classification models. I've demonstrated the simplicity with which a GP model can be fit to continuous-valued data using `PyMC3`. Given the prevalence of non-linear relationships among variables in so many settings, Gaussian processes should be present in any applied statistician's toolkit. I often find myself, rather than building stand-alone GP models, including them as components in a larger hierararchical model, in order to adequately account for non-linear confounding variables such as age effects in biostatistical applications, or for function approximation in reinforcement learning tasks.\n",
    "\n",
    "You can readily implement such models using `scikit-learn`, GPflow, GPy, [Stan](http://mc-stan.org), [Edward](http://edwardlib.org) and [George](https://github.com/dfm/george), to name just a few of the more popular packages. I encourage you to try a few of them to get an idea of which fits in to your data science workflow best. "
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/05a9210d3ea6546e14e6fcb8fd10a906"
  },
  "gist": {
   "data": {
    "description": "GP Showdown.ipynb",
    "public": true
   },
   "id": "05a9210d3ea6546e14e6fcb8fd10a906"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
